
---
title: "Homework 3 for SDS, Fall 2022"
subtitle: "Two-Sample Testing"
author: "Cem Sirin & Sara Zeynalpour & Sophia Balestrucci & Vahid Ghanbarizadeh"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: paper
    toc: true
    toc_float: true
    toc_collapsed: false
    toc_depth: 4
---

## Task 1: Go Study!
## Task 2: Essay

Prompt: "Read and understand Friedman’s paper. Write a short essay (< 1 A4 page) to summarize it. Also explain the essence of the auxiliary tests adopted (i.e. Mann-Whitney or KolmogorovSmirnov) and why we use them and not others."

Paper: Friedman, J. H. (2003). On multivariate goodness–of–fit and two–sample testing. *Statistical Problems in Particle Physics, Astrophysics, and Cosmology, 1,* 311.

### Essay

Friedman (2003) introduces a new method for goodness–of–fit and two-sample testing. The article mentions that there plenty of existing powerful ways of testing in the univariate case. However, these tests loose power dramatically as the dimension (that is, the number of features) increases, also known as the curse of dimensionality. In order to overcome this problem, the author comes up with a clever idea to end up in a univariate case, even in higher dimensions. Briefly, the idea is to train a binary classification model to distinguish between the two samples. This trained binary classifier will assign a score to each sample, which is the confidence score of the sample belonging to the first sample. These scores are then used to construct a test statistic, which is a function of the scores.

Suppose we have two samples, $\{x_i\}^N_{i=1}$ and $\{z_i\}^M_{i=1}$, and we want to test whether they come from the same distribution. The idea is to construct a test statistic, $T$, that is a function of the samples. Then, we can test the null hypothesis that $T$ is distributed according to a certain distribution, say the standard normal distribution. The test statistic is constructed as follows:

## Task 3: Simulation Study

In order to test Friedman’s idea, we will setup a simulation study that will put it into practice. We will be using a logistic regression model to train a binary classifier. We will use the `glm` function from the `stats` package.

To carry on the with example given in the slides, we will be simulating the number of errors by students coding $\{x_i\}_{i=1}^N$ and $\{z_i\}_{i=1}^N$ in R vs Python. We will use a Poisson distribution to simulate the number of errors. To simulate tests with different powers and variance, we may tinker with many parameters, however, the outcomes are inline with the intuition. Therefore, we will only change the rate parameter of the Poisson distribution, $\lambda$, of the second sample, $\{z_i\}_{i=1}^N$. We will use $\lambda = 1.5$ for the first sample, $\{x_i\}_{i=1}^N$, and $\lambda = 2.5$ for the second sample, $\{z_i\}_{i=1}^N$. We will use $N = 20$ and $M = 1000$.


```{r Data simulation, message=FALSE, include=FALSE}
library(tidyverse)

# Function to simulate data
sim_data <- function(lambda_x, lambda_z, n, M) {
  # Initialize the data
  x <- matrix(rpois(M*n, lambda_x), nrow = M, ncol = n)
  z <- matrix(rpois(M*n, lambda_z), nrow = M, ncol = n)
  # Return the data
  return(list(x = x, z = z))
}
x <- data$x
z <- data$z
# Function to train the binary classifier
train_model <- function(x, z, M, perm = FALSE) {
  # Create binary labels, 1 for x and -1 for z
  y <- matrix(0, nrow = M, ncol = ncol(x) + ncol(z))
  y[,1:ncol(x)] <- 1

  # Permute the labels if needed
  if (perm) y <- t(apply(y, 1, sample))

  # Index of the 0 and 1 samples
  # idx_x <- matrix(NA, nrow = M, ncol = ncol(x))
  # idx_z <- matrix(NA, nrow = M, ncol = ncol(z))

  # for (i in 1:M) {
  #   idx_x[i,] <- which(y[i,] == 1)
  #   idx_z[i,] <- which(y[i,] == 0)
  # }

  ## replace for loop with apply
  idx_x <- t(apply(y, 1, function(x) which(x == 1)))
  idx_z <- t(apply(y, 1, function(x) which(x == 0)))

  # Initialize the scores
  scores <- matrix(NA, nrow = M, ncol = ncol(x) + ncol(z))

  # Train the binary classifier
  for (i in 1:M) {
    # Create the training data
    train <- cbind(c(x[i,], z[i,]), y[i,]) %>% as.data.frame()
    # Train the model
    classifier <- glm(V2 ~ V1, data = train, family = binomial(link = "logit"))
    # Calculate the scores
    scores[i,] <- classifier$fitted.values
  }

  # TODO: replace the for loop with apply
  # fit_values <- function(row) {
  #   train <- cbind(c(row$x, row$z), row$y) %>% as.data.frame()
  #   classifier <- glm(V2 ~ V1, data = train, family = binomial(link = "logit"))
  #   classifier$fitted.values
  # }

  # merged <- tibble(x = x, z = z, y = y)
  # scores_with_apply <- apply(merged, 1, fit_values)

  # merged[1,]$x
  # Return the scores, idx_x, and idx_z
  return(list(scores = scores, idx_x = idx_x, idx_z = idx_z))
}

# surpess warnings
# options(warn=-1)

# Function to calculate the test statistics
calc_stats <- function(scores, idx_x, idx_z, n, M) {
  # Initialize the test statistics
  t_stat <- matrix(NA, nrow = M, ncol = 2)
  # Calculate the test statistics
  for (i in 1:M) {
    t_stat[i,1] <- wilcox.test(scores[i,idx_x[i,]], scores[i,idx_z[i,]])$statistic
    t_stat[i,2] <- ks.test(scores[i,idx_x[i,]], scores[i,idx_z[i,]])$statistic
  }
  colnames(t_stat) <- c("Wilcox", "KS")
  # Return the test statistics
  return(list(wilcox = t_stat[,1], ks = t_stat[,2]))
}

# Function to iterate J permutations
perm_sim <- function(data, n, M, J) {
  # Initialize the test statistics
  wilcox <- matrix(NA, nrow = M, ncol = J)
  ks     <- matrix(NA, nrow = M, ncol = J)
  # Iterate over the permutations
  for (j in 1:J) {
    # Train the model
    scores <- train_model(x = data$x, z = data$z, M = M, perm = TRUE)
    # Calculate the test statistics
    stats  <- calc_stats(scores = scores$scores, idx_x = scores$idx_x, idx_z = scores$idx_z, n = n, M = M)
    wilcox[,j] <- stats$wilcox
    ks[,j]     <- stats$ks
  }
  # Return the test statistics
  return(list(wilcox = wilcox, ks = ks))
}
```

```{r warning=FALSE}
lambda_x <- 4
lambda_z_grid <- lambda_x + 0:4 / 2

n <- 20  # Sample size
M <- 20 # Number of simulations
J <- 20 # Number of permutations

pwilcox <- matrix(NA, nrow = M, ncol = length(lambda_z_grid))
pks     <- matrix(NA, nrow = M, ncol = length(lambda_z_grid))

for (i in 1:length(lambda_z_grid)) {
  print(paste("lambda_z =", lambda_z_grid[i]))
  data   <- sim_data(lambda_x = lambda_x, lambda_z = lambda_z_grid[i], n = n, M = M)
  scores <- train_model(x = data$x, z = data$z, M = M)
  stats  <- calc_stats(scores = scores$scores, idx_x = scores$idx_x, idx_z = scores$idx_z, n = n, M = M)
  results <- perm_sim(data = data, n = n, M = M, J = J)

  pwilcox[,i] <- rowSums(results$wilcox >= stats$wilcox) / J
  pks[,i]     <- rowSums(results$ks     >= stats$ks)     / J
}

library(ggplot2)
# function to plot boxplots for every column
boxplot_columns <- function(df, xlab = "x", ylab = "y", main = "main") {
  ggplot(stack(df), aes(x = ind, y = values)) + geom_boxplot() + labs(x = xlab, y = ylab, title = main)
}

# boxplot of the p-values for Wilcox
pwilcox <- data.frame(pwilcox)
colnames(pwilcox) <- lambda_z_grid
boxplot_columns(pwilcox, xlab = "lambda_z", ylab = "p-value", main = "p-values for the Wilcox test statistic")

ggplot(stack(pwilcox), aes(x = ind, y = values)) + geom_boxplot() + labs(x = "lambda_z", y = "p-value", title = "p-values for the Wilcox test statistic")
# save the plot
ggsave("wilcox.png", width = 10, height = 5)

# boxplot of the p-values for KS
pks <- data.frame(pks)
colnames(pks) <- lambda_z_grid
boxplot_columns(pks, xlab = "lambda_z", ylab = "p-value", main = "p-values for the KS test statistic")
```

## Task 4: Case Study

## Loading the data from the previous homework
```{r Data processing}
# Load the data
load("hw2_data.RData")

# Standardize the data
asd <- lapply(asd_sel, function(x) as.data.frame(x))
td  <- lapply(td_sel, function(x) as.data.frame(x))

# function to scale variation to 1 assuming 0 mean
scale_0mean <- function(x) {
  n <- nrow(x)                    # number of observations
  x <- as.matrix.data.frame(x)
  sdv <- sqrt(diag(t(x) %*% x)/n) # standard deviation
  x <- t(t(x) / sdv)              # divide by the standard deviation
  return(as.data.frame(x))
}

# scale the datasets
asd <- lapply(asd, scale_0mean)
td  <- lapply(td,  scale_0mean)

# Pool the data
asd <- do.call(rbind, asd)
td  <- do.call(rbind, td)
```


## References

- Craddock, C., Benhajali, Y., Chu, C., Chouinard, F., Evans, A., Jakab, A., ... & Bellec, P. (2013). The neuro bureau preprocessing initiative: open sharing of preprocessed neuroimaging data and derivatives. *Frontiers in Neuroinformatics, 7,* 27.
- Friedman, J. H. (2003). On multivariate goodness–of–fit and two–sample testing. *Statistical Problems in Particle Physics, Astrophysics, and Cosmology, 1,* 311.

## Appendix

```{r Appendix 1, message=FALSE}
library(tidyverse)

# Function to simulate data
sim_data <- function(lambda_x, lambda_z, n, M) {
  # Initialize the data
  x <- matrix(rpois(M*n, lambda_x), nrow = M, ncol = n)
  z <- matrix(rpois(M*n, lambda_z), nrow = M, ncol = n)
  # Return the data
  return(list(x = x, z = z))
}
# Function to train the binary classifier
train_model <- function(x, z, M, perm = FALSE) {
  # Create binary labels, 1 for x and -1 for z
  y <- matrix(0, nrow = M, ncol = ncol(x) + ncol(z))
  y[,1:ncol(x)] <- 1

  # Permute the labels if needed
  if (perm) y <- t(apply(y, 1, sample))

  # Index of the 0 and 1 samples
  idx_x <- matrix(NA, nrow = M, ncol = ncol(x))
  idx_z <- matrix(NA, nrow = M, ncol = ncol(z))

  for (i in 1:M) {
    idx_x[i,] <- which(y[i,] == 1)
    idx_z[i,] <- which(y[i,] == 0)
  }

  # Initialize the scores
  scores <- matrix(NA, nrow = M, ncol = ncol(x) + ncol(z))

  # Train the binary classifier
  for (i in 1:M) {
    # Create the training data
    train <- cbind(c(x[i,], z[i,]), y[i,]) %>% as.data.frame()
    # Train the model
    classifier <- glm(V2 ~ V1, data = train, family = binomial(link = "logit"))
    # Calculate the scores
    scores[i,] <- classifier$fitted.values
  }
  # Return the scores, idx_x, and idx_z
  return(list(scores = scores, idx_x = idx_x, idx_z = idx_z))
}

# surpess warnings
# options(warn=-1)

# Function to calculate the test statistics
calc_stats <- function(scores, idx_x, idx_z, n, M) {
  # Initialize the test statistics
  t_stat <- matrix(NA, nrow = M, ncol = 2)
  # Calculate the test statistics
  for (i in 1:M) {
    t_stat[i,1] <- wilcox.test(scores[i,idx_x[i,]], scores[i,idx_z[i,]])$statistic
    t_stat[i,2] <- ks.test(scores[i,idx_x[i,]], scores[i,idx_z[i,]])$statistic
  }
  colnames(t_stat) <- c("Wilcox", "KS")
  # Return the test statistics
  return(list(wilcox = t_stat[,1], ks = t_stat[,2]))
}

# Function to iterate J permutations
perm_sim <- function(data, n, M, J) {
  # Initialize the test statistics
  wilcox <- matrix(NA, nrow = M, ncol = J)
  ks     <- matrix(NA, nrow = M, ncol = J)
  # Iterate over the permutations
  for (j in 1:J) {
    # Train the model
    scores <- train_model(x = data$x, z = data$z, M = M, perm = TRUE)
    # Calculate the test statistics
    stats  <- calc_stats(scores = scores$scores, idx_x = scores$idx_x, idx_z = scores$idx_z, n = n, M = M)
    wilcox[,j] <- stats$wilcox
    ks[,j]     <- stats$ks
  }
  # Return the test statistics
  return(list(wilcox = wilcox, ks = ks))
}
```
