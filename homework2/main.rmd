
---
title: "Homework 2 for SDS, Fall 2022"
author: "Cem Sirin & Sophia Balestrucci & Vahid Ghanbarizadeh"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: false
    toc_depth: 3
---

## Task 1: Pooling Subjects

Our first task is to pool the data for the two subject groups. The fMRI data that we have contains time series for each subject and each Region of Interest (ROI). We are interested in estimating the correlation between ROIs. Our first approach is to estimate the correlation between ROIs for each subject and then average the correlations across subjects. Our reasoning is that

a. The subjects may not be synchronized in time with regards to the cognitive or motor task that they are performing. Therefore, we believe that estimating the correlation per subject and then averaging the correlations across subjects is more robust. 
b. Moreover, a neural activity causes a *hemodynamic response function* (HRF) which is a time-varying function that affects the blood oxygen levels. The HRF is not synchronized across subjects. Therefore, we believe that pooling the data across subjects is more robust.

When it comes to processing the data of an individual, we would like to address a couple issues. First of all, we are dealing with **Pearson correlation**, which means that methods such as demeaning or standardizing the data will not affect the correlation. 

## Task 2: Estimating the Correlation Matrix

Now, let's exactly do what we have described above.

```{r}
# Load the data
# setwd("homework2")
load("hw2_data.RData")

# Get the correlation matrix for each subject
R.asd <- lapply(asd_sel, cor)
R.td  <- lapply(td_sel,  cor)

# Average the correlation matrices across subjects
R.asd.mean <- matrix(0, nrow = 116, ncol = 116)
R.td.mean  <- matrix(0, nrow = 116, ncol = 116)

for (r in R.asd) R.asd.mean <- R.asd.mean + r
for (r in R.td)  R.td.mean  <- R.td.mean  + r

R.asd.mean <- R.asd.mean / length(R.asd)
R.td.mean  <- R.td.mean  / length(R.td)

```

In, order to estimate the association graphs $\widehat{\mathcal{G}}^{\mathrm{ASD}}(t)$ and $\widehat{\mathcal{G}}^{\mathrm{TD}}(t)$, we will to construct a confidence interval of 95% for each correlation coefficient. We are interested if the absolute value of the correlation coefficient is greater than $t$ therefore we will use a **single-sided** confidence interval.

Let's start with performing the **Fisher Z transformation** on the correlation matrices to make them more Gaussian. We will also use the **Bonferonni correction** to correct for multiple testing.

```{r}
asso_graph <- function(R, alpha = 0.05, n=145, tau = 0.5, bonf = TRUE) {
  z <- abs(atanh(R))                    # Fisher Z transformation
  z.sigma <- 1 / sqrt(n - 3)            # variance fisher z transformation
  p <- nrow(z)                          # number of ROIs
  m <- if (bonf) p * (p - 1) / 2 else 1 # number of hypothesis (edges in the graph)
  q <- qnorm(1-alpha/m)                 # quantile of the standard normal distribution
  tau <- atanh(tau)                       # threshold (transformed)
  G <- 1 * (z > tau + q * z.sigma)        # construct the association graph
  diag(G) <- 0                          # set the diagonal to zero
  return(G)
}

G.asd <- asso_graph(R.asd.mean, tau = 0.1)
G.td  <- asso_graph(R.td.mean, tau = 0.1)
paste("Number of edges in the ASD and TD graphs using Bonferonni correction:", 
      sum(G.asd) / 2, sum(G.td) / 2)
```

We are also going to implement the **Holm-Bonferonni method** to correct for multiple testing, which is a less conservative method than the Bonferonni correction.

```{r}
asso_graph_HB <- function(R, alpha = 0.05, n=145, tau = 0.5) {
  z <- abs(atanh(R))                    # Fisher Z transformation
  z.sigma <- 1 / sqrt(n - 3)            # variance fisher z transformation
  p <- nrow(z)                          # number of ROIs
  m <- p * (p - 1) / 2                  # number of hypothesis (edges in the graph)
  q <- qnorm(1-alpha/m)                 # quantile of the standard normal distribution
  tau <- atanh(tau)                     # threshold (transformed)

  # Get off-diagonal elements (transformed correlation coefficients)
  z_array <- z[lower.tri(z, diag = FALSE)]
  z_idx <- which(lower.tri(z, diag = FALSE))

  # Order from largest to smallest
  z_idx <- z_idx[order(z_array, decreasing = TRUE)]
  z_array <- z_array[order(z_array, decreasing = TRUE)]

  # Test the largest correlation coefficient and update qnorm
  largest <- z_array[1];
  
  k <- 1
  while (largest > tau + q * z.sigma && k < length(z_array)) {
    k <- k + 1
    largest <- z_array[k]
    q <- qnorm(1 - 0.05 / (m - k + 1))
  }

  # Construct the graph
  G <- matrix(0, nrow = 116, ncol = 116)
  G[z_idx[1:(k-1)]] <- 1           # Add the edges to lower triangle
  G <- G + t(G)                    # Add the edges to upper triangle

  return(G)
}

G.asd.HB <- asso_graph_HB(R.asd.mean, tau = 0.1)
G.td.HB  <- asso_graph_HB(R.td.mean, tau = 0.1)

paste("Number of edges in the ASD and TD graphs using Holm-Bonferonni method:", 
      sum(G.asd.HB) / 2, sum(G.td.HB) / 2)

```

OK, it did not make much of a difference. Maybe we'll remove this part. We need to choose a threshold $\tau$ by trial and error. Uhmm let's do that later, now we do task 3.

## Task 3: Comparing the Association Graphs

In this part, we will compare the two association graphs. In order to do so, we will use the packages `igraph` and `ggraph` to plot the graphs and `ggnetwork` to compare them.

```{r}
library(ggraph, quietly = TRUE)
library(tidygraph, quietly = TRUE)

load("neuro.aal.rda")

neuro.aal <- levels(neuro.aal$desc$label)
# split desc with "_" and take the first element
superregion <- sapply(neuro.aal, function(x) strsplit(x, "_")[[1]][1])


as_tbl_graph(G.asd * R.asd.mean, directed = FALSE) %>%
  ggraph(layout = "circle") +
  geom_edge_link(aes(color = weight), alpha = 0.5) +
  scale_edge_colour_distiller(palette = "PuRd", direction = 1) +
  geom_node_point(aes(color = superregion), size = 3) +
  geom_node_text(aes(label = name), repel = TRUE) +
  labs(title = "ASD Association Graph") +
  theme_void()

as_tbl_graph(G.td * R.td.mean, directed = FALSE) %>%
  ggraph(layout = "circle") +
  geom_edge_link(aes(color = weight), alpha = 0.5) +
  scale_edge_colour_distiller(palette = "PuRd", direction = 1) +
  geom_node_point(aes(color = superregion), size = 3) +
  geom_node_text(aes(label = name), repel = TRUE) +
  labs(title = "TD Association Graph") +
  theme_void()

```

## Task 4: Partial Correlation

In this part, we will use the partial correlation to remove the effect of the other ROIs on the correlation between two ROIs. We will use the `pcor` function from the `ppcor` package to compare our code with the function.

```{r}
library(ppcor, quietly = TRUE)

asd <- lapply(asd_sel, function(x) as.data.frame(scale(as.data.frame(x)))) %>% 
  data.table::rbindlist() %>% as.data.frame()

td <- lapply(td_sel, function(x) as.data.frame(scale(as.data.frame(x)))) %>% 
  data.table::rbindlist() %>% as.data.frame()
R.asd.pcor <- pcor(asd)
R.td.pcor <- pcor(td)

R.asd.pcor 
```

