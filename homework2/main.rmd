
---
title: "Homework 2 for SDS, Fall 2022"
author: "Cem Sirin & Sophia Balestrucci & Vahid Ghanbarizadeh"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: false
    toc_depth: 3
---

## Task 1: Pooling Subjects

Our first objective is to pool the data for the two subject groups. The fMRI data that we have contains time series for each subject and each Region of Interest (ROI). We are interested in estimating the correlation between ROIs. To do so, we will introduce our assumptions and their justifications.

1. For a given subject, the time series of the ROIs are independent and identically distributed (i.i.d.) with regards to time.
2. The fMRI data measures the change in the blood oxygen levels. Biologically, it is not possible for the blood oxygen levels to have an increasing/decreasing trend. Therefore, we assume that the mean is zero for each ROI.
3. The subjects vary in terms of biological and cognitive factors. Therefore, we assume that the variance is different for each ROI-subject pair. However, we will assume that the correlation between the ROIs is the same for each subject.
4. The subjects are independent of each other.

Based on these assumptions, we willl pool the data in the following way. First, we will standardize each ROI-subject pair by dividing by the standard deviation that is estimated assuming zero mean. Then, we can simply pool the data that will result in a matrix of size $1740 \times 116$.
  
```{r}
# Load the data
# setwd("homework2")
load("hw2_data.RData")

# Standardize the data
asd <- lapply(asd_sel, function(x) as.data.frame(x))
td  <- lapply(td_sel, function(x) as.data.frame(x))


scale_0mean <- function(x) {
  n <- nrow(x)                    # number of observations
  x <- as.matrix.data.frame(x)
  sdv <- sqrt(diag(t(x) %*% x)/n) # standard deviation
  x <- t(t(x) / sdv)              # divide by the standard deviation
  return(as.data.frame(x))
}

asd <- lapply(asd, scale_0mean)
td  <- lapply(td,  scale_0mean)


# Pool the data
asd <- do.call(rbind, asd)
td  <- do.call(rbind, td)

```


## Task 2: Estimating the Correlation Matrix

Now, let's exactly do what we have described above.

```{r}

# Correlation assuming 0 mean
cor_0mean <- function(x) {
  x <- as.matrix.data.frame(x)
  x <- t(x) %*% x
  x <- x / sqrt(diag(x) %o% diag(x))
  return(x)
}


# Get the correlation matrix for each subject
R.asd <- cor_0mean(asd)
R.td  <- cor_0mean(td)

```

In, order to estimate the association graphs $\widehat{\mathcal{G}}^{\mathrm{ASD}}(t)$ and $\widehat{\mathcal{G}}^{\mathrm{TD}}(t)$, we will to construct a confidence interval of 95% for each correlation coefficient. We are interested if the absolute value of the correlation coefficient is greater than $t$ therefore we will use a **single-sided** confidence interval.

Let's start with performing the **Fisher Z transformation** on the correlation matrices to make them more Gaussian. We will also use the **Bonferonni correction** to correct for multiple testing.

```{r}
asso_graph <- function(R, alpha = 0.05, n=145, tau = 0.5, bonf = TRUE) {
  z <- abs(atanh(R))                    # Fisher Z transformation
  z.sigma <- 1 / sqrt(n - 3)            # variance fisher z transformation
  p <- nrow(z)                          # number of ROIs
  m <- if (bonf) p * (p - 1) / 2 else 1 # number of hypothesis (edges in the graph)
  q <- qnorm(1-alpha/m)                 # quantile of the standard normal distribution
  tau <- atanh(tau)                       # threshold (transformed)
  G <- 1 * (z > tau + q * z.sigma)        # construct the association graph
  diag(G) <- 0                          # set the diagonal to zero
  return(G)
}

G.asd <- asso_graph(R.asd, tau = 0.1)
G.td  <- asso_graph(R.td, tau = 0.1)
paste("Number of edges in the ASD and TD graphs using Bonferonni correction:", 
      sum(G.asd) / 2, sum(G.td) / 2)
```

We are also going to implement the **Holm-Bonferonni method** to correct for multiple testing, which is a less conservative method than the Bonferonni correction.

```{r}
asso_graph_HB <- function(R, alpha = 0.05, n=145, tau = 0.5) {
  z <- abs(atanh(R))                    # Fisher Z transformation
  z.sigma <- 1 / sqrt(n - 3)            # variance fisher z transformation
  p <- nrow(z)                          # number of ROIs
  m <- p * (p - 1) / 2                  # number of hypothesis (edges in the graph)
  q <- qnorm(1-alpha/m)                 # quantile of the standard normal distribution
  tau <- atanh(tau)                     # threshold (transformed)

  # Get off-diagonal elements (transformed correlation coefficients)
  z_array <- z[lower.tri(z, diag = FALSE)]
  z_idx <- which(lower.tri(z, diag = FALSE))

  # Order from largest to smallest
  z_idx <- z_idx[order(z_array, decreasing = TRUE)]
  z_array <- z_array[order(z_array, decreasing = TRUE)]

  # Test the largest correlation coefficient and update qnorm
  largest <- z_array[1];
  
  k <- 1
  while (largest > tau + q * z.sigma && k < length(z_array)) {
    k <- k + 1
    largest <- z_array[k]
    q <- qnorm(1 - 0.05 / (m - k + 1))
  }

  # Construct the graph
  G <- matrix(0, nrow = 116, ncol = 116)
  G[z_idx[1:(k-1)]] <- 1           # Add the edges to lower triangle
  G <- G + t(G)                    # Add the edges to upper triangle

  return(G)
}

G.asd.HB <- asso_graph_HB(R.asd, tau = 0.1)
G.td.HB  <- asso_graph_HB(R.td, tau = 0.1)

paste("Number of edges in the ASD and TD graphs using Holm-Bonferonni method:", 
      sum(G.asd.HB) / 2, sum(G.td.HB) / 2)

```

OK, it did not make much of a difference. Maybe we'll remove this part. We need to choose a threshold $\tau$ by trial and error. Uhmm let's do that later, now we do task 3.

## Task 3: Comparing the Association Graphs

In this part, we will compare the two association graphs. In order to do so, we will use the packages `igraph` and `ggraph` to plot the graphs and `ggnetwork` to compare them.

```{r}
library(ggraph, quietly = TRUE)
library(tidygraph, quietly = TRUE)

load("neuro.aal.rda")

neuro.aal <- levels(neuro.aal$desc$label)
# split desc with "_" and take the first element
superregion <- sapply(neuro.aal, function(x) strsplit(x, "_")[[1]][1])


as_tbl_graph(G.asd * R.asd, directed = FALSE) %>%
  ggraph(layout = "circle") +
  geom_edge_link(aes(color = weight), alpha = 0.5) +
  scale_edge_colour_distiller(palette = "PuRd", direction = 1) +
  geom_node_point(aes(color = superregion), size = 3) +
  geom_node_text(aes(label = name), repel = TRUE) +
  labs(title = "ASD Association Graph") +
  theme_void()

as_tbl_graph(G.td * R.td, directed = FALSE) %>%
  ggraph(layout = "circle") +
  geom_edge_link(aes(color = weight), alpha = 0.5) +
  scale_edge_colour_distiller(palette = "PuRd", direction = 1) +
  geom_node_point(aes(color = superregion), size = 3) +
  geom_node_text(aes(label = name), repel = TRUE) +
  labs(title = "TD Association Graph") +
  theme_void()

```

## Task 4: Partial Correlation

In this part, we will use the partial correlation to remove the effect of the other ROIs on the correlation between two ROIs. We will use the `pcor` function from the `ppcor` package to compare our code with the function.

```{r}
library(ppcor, quietly = TRUE)

R.asd.pcor <- pcor(asd)
R.td.pcor <- pcor(td)

R.asd.pcor 


```

