
---
title: "Homework 1 for SDS, Fall 2022"
author: "Cem Sirin & Vahid Ghanbarizadeh"
date: "`r Sys.Date()`"
---

## Exercise 1: Stat4Race (2nd ed.)

## Exercise 2: Mind your own biz. . .

### Setting up the landscape

We want to be sure that we have the necessary functions before we start answering the questions. We will apply the Perturbed Histogram (Dwork et al., 2006) to privatize the data. The process starts with dividing the data into $m$ bins, and follows by adding noise to each bin count. Below is the function `perturb`, that takes the data, the number of bins, and the variance parameter $\epsilon$ as inputs, and returns the PMF for the original and perturbed data.The function `laplace` is to generate Laplacian noise. Lastly, the function `ePDF` constructs pdf functions from the probability mass functions obtained from the perturbed histogram.

```{r}
rlaplace <- function(n, mu = 0, b = 1) {
  r <- runif(n, 0, 1)
  r <- mu - b * sign(r - 0.5) * log(1 - 2 * abs(r - 0.5))
  return(r)
}

perturb <- function(x, epsilon, breaks) {

  h <- hist(x, breaks = breaks, plot = FALSE)
  counts <- h$counts
  p_hat <- counts / sum(counts) * length(counts)

  # add noise to each bin
  counts <- counts + rlaplace(length(counts), 0, 8 / epsilon^2)
  # count 0 or max
  counts[counts < 0] <- 0 
  q_hat <- counts / sum(counts) * length(counts)

  # return $p_hat and $q
  return(list(p_hat = p_hat, q_hat = q_hat))
}

ePDF <- function(x, pmf, breaks) {
  # An m by n matrix to compare x with break points
  breaks_matrix <- matrix(breaks, nrow = length(x), ncol = length(breaks), byrow = TRUE)

  # An n-dimensional vector to store which bin each x falls into
  bins <- rowSums(breaks_matrix < x)

  # Assign the probability of each bin to the corresponding x
  d <- pmf[bins]

  # if x is outside the range of breaks, then d = 0
  d[is.na(d)] <- 0 

  return(d)
}
```

To test the functions, let's generate a random sample from a uniform distribution with mean 0 and standard deviation 1, and apply the perturbed histogram to it. We will use 10 bins, and $\epsilon = 0.1$.

```{r}
set.seed(123)

test1 <- perturb(runif(1000), 0.5, 0:10 / 10)
# Two barplots side by side
par(mfrow = c(1, 2))
barplot(test1$p_hat, main = "Original", col = "lightblue")
barplot(test1$q_hat, main = "Perturbed", col = "coral")

```



### Exercise 2.1

To compare the two approximations we are instructed to use Mean Integrated Squared Error (MISE). The MISE is defined as

$$
\text{MISE}(\hat{p}, p) = \mathbb{E}\left[ \int_{0}^{1} (p(x) - \hat{p}(x))^2 \, \mathrm{d}x \right]
$$

where $\hat{p}(i)$ is the probability of the $i$th bin of the approximate histogram, and $p(i)$ is the probability of the $i$th bin of the original histogram. Below is the function `mise` that takes the original and approximated PDFs as inputs, and returns MISE. And, below that is the function `sim_mise` where given the number of simulations (`S`), epsilon (`eps`), number of bins (`m`), and the true distrubution `rdist, ddist`; simulates data generation, perturbization, and the calculation of MISE scores. In the end, it returns the average MISE score. 


```{r}
mise <- function(original, approx, start, end) {
  integrand <- function(x) (original(x) - approx(x))^2
  return(integrate(integrand, start, end, subdivisions = 1000, stop.on.error = FALSE)$value)
}

sim_mise <- function(S, eps, m, n, rdist, ddist) {
  # S: number of simulations
  # eps: epsilon
  # m: number of bins
  # n: sample size
  # rdist: random distribution
  # ddist: true distribution
  breaks <- 0:m / m

  p_mises <- rep(NA, S)
  q_mises <- rep(NA, S)
  for (i in 1:S) {
    # generate data
    x <- rdist(n)
    # get densities
    res <- perturb(x, eps, breaks)
    # compute MISE
    p_mises[i] <- mise(ddist, function(x) ePDF(x, res$p_hat, breaks), 0, 1)
    q_mises[i] <- mise(ddist, function(x) ePDF(x, res$q_hat, breaks), 0, 1)
  }
  return(list(p_mises = mean(p_mises), q_mises = mean(q_mises)))
}
```


Now, it is time to simulate the entire shebang. We are using the given grids for the parameters.

```{r}
set.seed(123)
S <- 40 # We will change this to 1000 later
n_grid <- c(100, 1000)
eps_grid <- c(0.1, 0.001)
m_grid <- 1:10 * 10

# create data frame to store results
scores <- data.frame(matrix(NA, nrow = length(n_grid) * length(eps_grid) * length(m_grid), ncol = 5))
colnames(scores) <- c("n", "eps", "m", "p_mise", "q_mise")

rdist <- function(n) rbeta(n, 10, 10)
ddist <- function(x) dbeta(x, 10, 10)

# loop over n_grid, eps_grid and m_grid
i <- 1
for (n in 1:length(n_grid)) {
  for (eps in 1:length(eps_grid)) {
    for (m in 1:length(m_grid)) {
      # compute MISE scores for p_hat and q
      res <- sim_mise(S, eps_grid[eps], m_grid[m], n_grid[n], rdist, ddist)
      scores[i, ] <- c(n_grid[n], eps_grid[eps], m_grid[m], res$p_mises, res$q_mises)
      i <- i + 1
    }
  }
}

```

Let's plot the MISE scores for $\hat{p}$ and $q$.

```{r}

library(ggplot2)
# TODO: plotting p_hat for diferent epsilons is stupid change that

# plot MISE scores for p_hat, with multiple lines for different pairs of n and eps
ggplot(scores, aes(x = m, y = p_mise, color = paste(n, eps))) + 
  geom_line() + 
  scale_color_discrete(name = "n, eps") + 
  labs(x = "Number of bins", y = "MISE", title = "MISE scores for p_hat")

# plot MISE scores for q, with multiple lines for different pairs of n and eps
ggplot(scores, aes(x = m, y = q_mise, color = paste(n, eps))) + 
  geom_line() + 
  scale_color_discrete(name = "n, eps") + 
  labs(x = "Number of bins", y = "MISE", title = "MISE scores for q")

```

### Exercise 2.2: Mixed distrubution
Let's plot a mixed beta distribution.

```{r}
rmbeta <- function(n, alphas, betas, probs) {
  if (length(alphas) != length(betas) | length(alphas) != length(probs)) {
    stop("alphas, betas and probs must have the same length")
  }
  # compute the density of a mixed beta distribution
  z <- sample(1:length(alphas), n, replace = TRUE, prob = probs)
 return(rbeta(n, alphas[z], betas[z]))
}

dmbeta <- function(x, alphas, betas, probs) {
  if (length(alphas) != length(betas) | length(alphas) != length(probs)) {
    stop("alphas, betas and probs must have the same length")
  }
  # compute the density of a mixed beta distribution
  return(rowSums(t(t(sapply(1:length(alphas), function(i) dbeta(x, alphas[i], betas[i]))) * probs)))
}



alphas <- c(2, 8)
betas <- c(8, 2)
probs <- c(0.3, 0.7)


# generate data
x <- rmbeta(1000, alphas, betas, probs)
# plot lightblue
hist(x, breaks = 20, freq = FALSE, main = "Mixed beta distribution", col = "lightblue", xlim = c(0, 1))
# add density
curve(dmbeta(x, alphas, betas, probs), add = TRUE, col = "red")

```

```{r}

S <- 2 # number of simulations
n_grid <- c(100, 1000)
eps_grid <- c(0.1)
m_grid <- 1:10 * 5

# create 3D array to store MISE scores
A <- array(NA, dim = c(length(n_grid), length(eps_grid), length(m_grid)), dimnames = list(n_grid, eps_grid, m_grid))
p_hat_mise <- A
q_hat_mise <- A

# Distrubution parameters
alphas <- c(2, 8)
betas <- c(8, 2)
probs <- c(0.5, 0.5)

rdist <- function(n) rmbeta(n, alphas, betas, probs)
ddist <- function(x) dbeta(x, alphas, betas, probs)

# loop over n_grid, eps_grid and m_grid
for (n in 1:length(n_grid)) {
  for (eps in 1:length(eps_grid)) {
    for (m in 1:length(m_grid)) {
      # compute MISE scores for p_hat and q
      res <- sim_mise(S, eps_grid[eps], m_grid[m], n_grid[n], rdist, ddist)
      p_hat_mise[n, eps, m] <- res$p_mises
      q_hat_mise[n, eps, m] <- res$q_mises
    }
  }
}

```

Let's plot the MISE scores for $\hat{p}$ and $q$.

```{r}

# plot MISE scores for p_hat, x axis: m
plot(m_grid, p_hat_mise[1, 1, ], type = "l", xlab = "m", ylab = "MISE", main = "MISE scores for p_hat")

# plot MISE scores for q, x axis: m
plot(m_grid, q_hat_mise[1, 1, ], type = "l", xlab = "m", ylab = "MISE", main = "MISE scores for q")

```

### Exercise 2.3: Thinking very hard...

We have decided to export the number of steps taken by Cem from his iPhone. We would like Proffessor Brutti to give Cem some advice on running and maybe recommend him a new pair of running shoes since Brutti is has a deep knowledge of running shoes and Cem is quite slow. However, it is very dangerous for Brutti to know the number of steps taken by Cem since he might then extrapolate where Cem is running and set a trap for him, because Cem is quite annoying during lectures. 

```{r}
library(readr)
daily_steps <- read_csv("../data/daily_steps.csv", 
                        col_types = cols(date = col_date(format = "%Y-%m-%d"),
                                         value = col_integer()))

x <- daily_steps$value
# Number of bins
m      <- 20
breaks <- (0:m / m) * (max(x) - min(x)) + min(x)

# Let's see the distrubution
hist(x, breaks=breaks, main="Cem's daily steps after his arrival in Rome",
     col = 'wheat1')
```

We will assume that Cem can take at most 27,000 steps in a day, and normalize our data between 0 and 1. Now we will perturb the data before Cem falls into a trap. We would like choose optimal tuning parameters. 

As we increase number of bins $m$ we gain information by knowing the sample distribution in more detail. However, we add more noise to the data, therefore we must optimize this trade-off. 

Wasserman and Zhou (2010) suggests to choose $n^{1/3}$ bins. In the previous section we can see that 5 bins attained the best score for $n=100$, and maybe speculate that 10 bins was also the best for $n=1000$. This makes their suggestion more believable. Since we have extracted Cem's data only after his arrival in Rome, that ammounts to 67 days. So we expect 4 bins to be the optimal number of bins. 

They also suggest that the increase in $k$, i.e., the size of the private dataset we are going to hand over to Brutti, does not affect the differential privacy. So we will numerically test the relationship between the MISE score and $k$.

Lastly, we find $\epsilon=0.4$ to be satisfactory for our privacy requirements. The real data will be at most $e^{0.4} \approx 1.5$ more likely than any of its neighbours. And, Rome is quite so Brutti has to cover a lot of ground to catch Cem. 

```{r}

eps <- 0.4
x <- daily_steps$value / 27000
m <- 4
breaks <- (0:m / m)

res <- perturb(x, eps, breaks)
res
# Two barplots side by side
par(mfrow = c(1, 2))
# The values of the bins
barplot(res$p_hat, col = 'wheat1', main = 'Values of p-hat', space=0)
barplot(res$q_hat, col = 'wheat2', main = 'Values of q-hat', space=0)

dq_hat <- function(x) ePDF(x, res$q_hat, breaks)

# To sample from the distrubution of q-hat, we will use
# twice uniform sampling method, which i don't remember the name of.
rq_hat <- function(n, pmf) {
  u1 <- runif(n) # which bin does it belong to?
  u2 <- runif(n) # The value that it will inside the bin
  m <- length(pmf)

  bmatrix <- matrix(cumsum(pmf) / m, 
                    nrow = n, ncol = m, byrow = TRUE)

  bins <- rowSums(u1 > bmatrix)
  bins / m + u2 / m
}

z <- rq_hat(1000, res$q_hat)

breaksZ <- (0:20 / 20)
# Let's see the distrubution
par(mfrow = c(1, 2))
hist(x, breaks=breaksZ, main="Cem's daily steps after his arrival in Rome",
     col = 'cornsilk')
hist(z, breaks=breaksZ, main="Cem's privaized daily steps after his arrival in Rome",
     col = 'lightblue')

```

Now let's see how similar is summary statistics of the original data and the perturbed data, for different values of $k$.

```{r}
M <- 1000
k <- c(100, 250, 500, 1000, 2500, 5000, 10000, 25000, 50000, 100000)

meanX <- mean(x)
sdX <- sd(x)

mean_diff <- array(NA, length(k), dimnames = list(k))
sd_diff   <- array(NA, length(k), dimnames = list(k))

for (i in 1:length(k)) {
  mean_diff[i] <- mean(replicate(M, (mean(rq_hat(k[i])) - meanX)^2))
  sd_diff[i]   <- mean(replicate(M, (sd(rq_hat(k[i])) - sdX)^2))
}

par(mfrow = c(1, 2))
plot(k, mean_diff, type = "l", xlab = "k", ylab = "Mean difference", main = "MSE of difference in means")
plot(k, sd_diff, type = "l", xlab = "k", ylab = "SD difference", main = "MSE of difference in SDs")

```

Our simulations show after that $k=1000$ the MSE for the mean does not improve, and after $k=500$ the MSE for the SD does not improve. So we do not see any advantage in increasing $k$ beyond 1000, therefore we will use $k=1000$.

### Excercise 2.4: Proving $\epsilon$-privacy

We will prove that the Perturbed Histogram satisfies $\epsilon$-differential privacy by simulating the algorithm with different values of $\epsilon$ and comparing the results. 

```{r}
eps_grid <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
n <- 1000
m <- n^(1/3)
breaks <- (0:m / m)

# Generate the data
x  <- runif(n)
# Neighbouring data
xx <- x
xx[which(x == min(x))] <- 0.999999


# Perturb the data
for (eps in eps_grid) {
  res <- perturb(x, eps, breaks)
  z <- rq_hat(1000, res$q_hat)

}

```